{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "c34a0368",
   "metadata": {},
   "source": [
    "Реализуйте нейронную сеть для метода DQN на доске для крестиков-ноликов. Не буду ограничивать фантазию, но кажется, что свёртки 3х3 здесь должны неплохо работать (в том числе обобщаться на доски размера побольше).\n",
    "Реализуйте DQN с нейронной сетью, обучите стратегии крестиков и ноликов. Замечание: скорее всего, experience replay потребуется сразу же.\n",
    "Реализуйте Double DQN и/или Dueling DQN."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "d14d14e2",
   "metadata": {},
   "outputs": [],
   "source": [
    "import gym\n",
    "from gym import make\n",
    "import numpy as np\n",
    "import torch\n",
    "from torch import nn\n",
    "from torch.nn import functional as F\n",
    "from torch.optim import Adam\n",
    "from collections import deque\n",
    "import random\n",
    "import os\n",
    "import copy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "269ad485",
   "metadata": {},
   "outputs": [],
   "source": [
    "N_ROWS, N_COLS, N_WIN = 3, 3, 3\n",
    "\n",
    "class TicTacToe(gym.Env):\n",
    "    def __init__(self, n_rows=N_ROWS, n_cols=N_COLS, n_win=N_WIN):\n",
    "        self.n_rows = n_rows\n",
    "        self.n_cols = n_cols\n",
    "        self.n_win = n_win\n",
    "\n",
    "        self.board = np.zeros((self.n_rows, self.n_cols), dtype=int)\n",
    "        self.gameOver = False\n",
    "        # ход первого игрока\n",
    "        self.curTurn = 1\n",
    "        self.emptySpaces = None\n",
    "        \n",
    "        self.reset()\n",
    "\n",
    "    def getEmptySpaces(self):\n",
    "        if self.emptySpaces is None:\n",
    "            res = np.where(self.board == 0)\n",
    "            self.emptySpaces = np.array([ (i, j) for i,j in zip(res[0], res[1]) ])\n",
    "        return self.emptySpaces\n",
    "\n",
    "    def makeMove(self, player, i, j):\n",
    "        self.board[i, j] = player\n",
    "        self.emptySpaces = None\n",
    "\n",
    "    def _check_terminal(self, cur_p):\n",
    "        cur_marks = np.where(self.board == cur_p)\n",
    "        for i,j in zip(cur_marks[0], cur_marks[1]):\n",
    "            if i <= self.n_rows - self.n_win:\n",
    "                if np.all(self.board[i:i+self.n_win, j] == cur_p):\n",
    "                    return True\n",
    "            if j <= self.n_cols - self.n_win:\n",
    "                if np.all(self.board[i,j:j+self.n_win] == cur_p):\n",
    "                    return True\n",
    "            if i <= self.n_rows - self.n_win and j <= self.n_cols - self.n_win:\n",
    "                if np.all(np.array([ self.board[i+k,j+k] == cur_p for k in range(self.n_win) ])):\n",
    "                    return True\n",
    "            if i <= self.n_rows - self.n_win and j >= self.n_win-1:\n",
    "                if np.all(np.array([ self.board[i+k,j-k] == cur_p for k in range(self.n_win) ])):\n",
    "                    return True\n",
    "        return False\n",
    "    \n",
    "    def isTerminal(self):\n",
    "        # проверим, не закончилась ли игра\n",
    "        cur_win = self._check_terminal(self.curTurn)\n",
    "        if cur_win:\n",
    "                self.gameOver = True\n",
    "                return self.curTurn\n",
    "            \n",
    "        if len(self.getEmptySpaces()) == 0:\n",
    "            self.gameOver = True\n",
    "            return 0\n",
    "\n",
    "        self.gameOver = False\n",
    "        return None\n",
    "\n",
    "    def getWinner(self):\n",
    "        # фактически запускаем isTerminal два раза для крестиков и ноликов\n",
    "        if self._check_terminal(1):\n",
    "            return 1\n",
    "        if self._check_terminal(-1):\n",
    "            return -1\n",
    "        if len(self.getEmptySpaces()) == 0:\n",
    "            return 0\n",
    "        return None\n",
    "    \n",
    "    def printBoard(self):\n",
    "        for i in range(0, self.n_rows):\n",
    "            print('----'*(self.n_cols)+'-')\n",
    "            out = '| '\n",
    "            for j in range(0, self.n_cols):\n",
    "                if self.board[i, j] == 1:\n",
    "                    token = 'x'\n",
    "                if self.board[i, j] == -1:\n",
    "                    token = 'o'\n",
    "                if self.board[i, j] == 0:\n",
    "                    token = ' '\n",
    "                out += token + ' | '\n",
    "            print(out)\n",
    "        print('----'*(self.n_cols)+'-')\n",
    "\n",
    "    def getState(self):\n",
    "        return self.board\n",
    "#         return (self.board, self.getEmptySpaces(), self.curTurn)\n",
    "\n",
    "    def action_from_int(self, action_int):\n",
    "        return int(action_int / self.n_cols), int(action_int % self.n_cols)\n",
    "\n",
    "    def int_from_action(self, action):\n",
    "        return action[0] * self.n_cols + action[1]\n",
    "    \n",
    "    def sample_action(self):\n",
    "        self.getEmptySpaces()\n",
    "        idx = np.random.randint(low=0, high=len(self.emptySpaces))\n",
    "        return self.int_from_action(self.emptySpaces[idx])\n",
    "    \n",
    "    def step(self, action):\n",
    "        action = self.action_from_int(action)\n",
    "        if self.board[action[0], action[1]] != 0:\n",
    "            return self.getState(), -10, True, {}, {}\n",
    "        self.makeMove(self.curTurn, action[0], action[1])\n",
    "        reward = self.isTerminal()\n",
    "        self.curTurn = -self.curTurn\n",
    "        return self.getState(), 0 if reward is None else reward, reward is not None, {}, {}\n",
    "\n",
    "    def reset(self):\n",
    "        self.board = np.zeros((self.n_rows, self.n_cols), dtype=int)\n",
    "        self.gameOver = False\n",
    "        self.emptySpaces = None\n",
    "        self.curTurn = 1\n",
    "        return self.board, {}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "c115454a",
   "metadata": {},
   "outputs": [],
   "source": [
    "class DQN:\n",
    "    def __init__(self, state_dim, action_dim):\n",
    "        self.steps = 0\n",
    "        self.model = NeuralNetwork(state_dim, action_dim)\n",
    "        self.model_target = copy.deepcopy(self.model)\n",
    "        self.replay_buffer = deque(maxlen=REPLAY_BUFFER_MAX_LEN)\n",
    "        self.device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "        self.model.to(self.device)\n",
    "        self.model_target.to(self.device)\n",
    "        self.optimizer = torch.optim.Adam(self.model.parameters(), lr=LEARNING_RATE)\n",
    "\n",
    "\n",
    "    def consume_transition(self, transition):\n",
    "        self.replay_buffer.append(transition)\n",
    "\n",
    "    def sample_batch(self):\n",
    "        states = []\n",
    "        actions = []\n",
    "        next_states = []\n",
    "        rewards = []\n",
    "        dones = []\n",
    "        for _ in range(BATCH_SIZE):\n",
    "            state, action, next_state, reward, done = random.choice(self.replay_buffer)\n",
    "            states.append(state)\n",
    "            actions.append(action)\n",
    "            next_states.append(next_state)\n",
    "            rewards.append(reward)\n",
    "            dones.append(done)\n",
    "            \n",
    "        return torch.Tensor(np.array(states)[:, np.newaxis, :, :]).to(self.device), \\\n",
    "            torch.Tensor(np.array(actions)).long(), \\\n",
    "            torch.Tensor(np.array(next_states)[:, np.newaxis, :, :]).to(self.device), \\\n",
    "            torch.Tensor(np.array(rewards)).to(self.device), \\\n",
    "            torch.Tensor(np.array(dones)).to(self.device)\n",
    "\n",
    "    def train_step(self, batch):\n",
    "        states, actions, next_states, rewards, dones = batch\n",
    "        out = self.model.forward(states)\n",
    "        with torch.no_grad():\n",
    "            out_target = self.model_target.forward(next_states)\n",
    "\n",
    "        max_a_target = out_target.max(dim=1)[0]\n",
    "        loss = torch.nn.functional.mse_loss(out[torch.arange(BATCH_SIZE), actions], rewards + GAMMA * max_a_target * (1 - dones).float())\n",
    "        self.optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        self.optimizer.step()\n",
    "        \n",
    "    def update_target_network(self):\n",
    "        self.model_target = copy.deepcopy(self.model)\n",
    "\n",
    "    def act(self, state, actions):\n",
    "        state = np.array(state)[np.newaxis, np.newaxis, :, :]\n",
    "        with torch.no_grad():\n",
    "            out = self.model.forward(torch.Tensor(state).to(self.device))\n",
    "        out = out.detach().cpu().numpy()\n",
    "        \n",
    "        mask = np.zeros(out.shape, dtype=bool)\n",
    "        mask[:, actions] = 1\n",
    "        \n",
    "        out[~mask] = -1e9\n",
    "        return out.argmax()\n",
    "\n",
    "    def update(self, transition):\n",
    "        self.consume_transition(transition)\n",
    "        if self.steps % STEPS_PER_UPDATE == 0:\n",
    "            batch = self.sample_batch()\n",
    "            self.train_step(batch)\n",
    "        if self.steps % STEPS_PER_TARGET_UPDATE == 0:\n",
    "            self.update_target_network()\n",
    "        self.steps += 1\n",
    "\n",
    "    def save(self):\n",
    "        torch.save(self.model.state_dict(), \"agent.pkl\")\n",
    "\n",
    "\n",
    "def evaluate_policy(agent, episodes=5, crosses=True):\n",
    "    max_steps = 1000\n",
    "    env = TicTacToe(n_rows=N_ROWS, n_cols=N_COLS, n_win=N_WIN)\n",
    "    returns = []\n",
    "    for _ in range(episodes):\n",
    "        done = False\n",
    "        state, _ = env.reset()\n",
    "        if not crosses:\n",
    "            possible_actions = list(map(env.int_from_action, env.getEmptySpaces()))\n",
    "            state, reward, done, _, _ = env.step(agent.act(state, possible_actions))\n",
    "\n",
    "        total_reward = 0.        \n",
    "        steps = 0\n",
    "        while not done and steps < max_steps:\n",
    "            possible_actions = list(map(env.int_from_action, env.getEmptySpaces()))\n",
    "            state, reward, done, _, _ = env.step(agent.act(state, possible_actions))\n",
    "            # ход соперника\n",
    "            if not done:\n",
    "                state, reward, done, _, _ = env.step(env.sample_action())\n",
    "                \n",
    "            total_reward += reward\n",
    "            steps += 1\n",
    "        returns.append(total_reward)\n",
    "    return returns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "f276a73d",
   "metadata": {},
   "outputs": [],
   "source": [
    "class NeuralNetwork(nn.Module):\n",
    "    def __init__(self, state_dim, action_dim, hidden_dim=64):\n",
    "        super(self.__class__, self).__init__()\n",
    "        self.model = nn.Sequential(\n",
    "            nn.Conv2d(in_channels=1, out_channels=hidden_dim, kernel_size=(3, 3)),\n",
    "            nn.Flatten(),\n",
    "            nn.Linear(hidden_dim, hidden_dim),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(hidden_dim, hidden_dim),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(hidden_dim, action_dim),\n",
    "            nn.Tanh()\n",
    "        )\n",
    "        \n",
    "    def forward(self, inp):       \n",
    "        out = self.model(inp)\n",
    "        return out"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c0bab6f3",
   "metadata": {},
   "source": [
    "# Учим крестики"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "67836203",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step: 10, Reward mean: 0.164, Reward std: 0.9365383067445772\n",
      "Step: 20, Reward mean: 0.188, Reward std: 0.9298688079508851\n",
      "Step: 30, Reward mean: 0.196, Reward std: 0.8795362414363607\n",
      "Step: 40, Reward mean: 0.294, Reward std: 0.8553151465980244\n",
      "Step: 50, Reward mean: 0.288, Reward std: 0.8631662644010133\n",
      "Step: 60, Reward mean: 0.242, Reward std: 0.8575756526394626\n",
      "Step: 70, Reward mean: 0.278, Reward std: 0.8250551496718264\n",
      "Step: 80, Reward mean: 0.336, Reward std: 0.8069101560892638\n",
      "Step: 90, Reward mean: 0.318, Reward std: 0.8251521071899411\n",
      "Step: 100, Reward mean: 0.136, Reward std: 0.8611062652193399\n",
      "Step: 110, Reward mean: 0.114, Reward std: 0.8677580307896897\n",
      "Step: 120, Reward mean: 0.138, Reward std: 0.8825848401145353\n",
      "Step: 130, Reward mean: 0.17, Reward std: 0.8491760712596652\n",
      "Step: 140, Reward mean: 0.182, Reward std: 0.8560817717951948\n",
      "Step: 150, Reward mean: 0.222, Reward std: 0.8465908102501467\n",
      "Step: 160, Reward mean: 0.214, Reward std: 0.8626725914273619\n",
      "Step: 170, Reward mean: 0.292, Reward std: 0.8335082483095173\n",
      "Step: 180, Reward mean: 0.276, Reward std: 0.853126016482911\n",
      "Step: 190, Reward mean: 0.51, Reward std: 0.7680494775728969\n",
      "Step: 200, Reward mean: 0.502, Reward std: 0.768111971004228\n",
      "Step: 210, Reward mean: 0.528, Reward std: 0.7597473264184612\n",
      "Step: 220, Reward mean: 0.546, Reward std: 0.758870212882282\n",
      "Step: 230, Reward mean: 0.692, Reward std: 0.6820087975972158\n",
      "Step: 240, Reward mean: 0.712, Reward std: 0.6580699051012742\n",
      "Step: 250, Reward mean: 0.636, Reward std: 0.7586197993725183\n",
      "Step: 260, Reward mean: 0.59, Reward std: 0.7961783719745218\n",
      "Step: 270, Reward mean: 0.484, Reward std: 0.8565885826929985\n",
      "Step: 280, Reward mean: 0.444, Reward std: 0.877988610404486\n",
      "Step: 290, Reward mean: 0.48, Reward std: 0.8611620056644395\n",
      "Step: 300, Reward mean: 0.414, Reward std: 0.898111351670827\n",
      "Step: 310, Reward mean: 0.422, Reward std: 0.8808609424875189\n",
      "Step: 320, Reward mean: 0.434, Reward std: 0.890866993439537\n",
      "Step: 330, Reward mean: 0.46, Reward std: 0.869712596206356\n",
      "Step: 340, Reward mean: 0.488, Reward std: 0.8613106292157319\n",
      "Step: 350, Reward mean: 0.396, Reward std: 0.8962053336150146\n",
      "Step: 360, Reward mean: 0.392, Reward std: 0.8844975975094562\n",
      "Step: 370, Reward mean: 0.398, Reward std: 0.8852095797041512\n",
      "Step: 380, Reward mean: 0.47, Reward std: 0.8585452812752511\n",
      "Step: 390, Reward mean: 0.744, Reward std: 0.6280636910377799\n",
      "Step: 400, Reward mean: 0.664, Reward std: 0.6863701625216528\n",
      "Step: 410, Reward mean: 0.626, Reward std: 0.6914651111950623\n",
      "Step: 420, Reward mean: 0.656, Reward std: 0.6705699068702681\n",
      "Step: 430, Reward mean: 0.644, Reward std: 0.673248839583107\n",
      "Step: 440, Reward mean: 0.576, Reward std: 0.7404215015786617\n",
      "Step: 450, Reward mean: 0.512, Reward std: 0.8401523671334861\n",
      "Step: 460, Reward mean: 0.476, Reward std: 0.861059812092052\n",
      "Step: 470, Reward mean: 0.336, Reward std: 0.9116490552838851\n",
      "Step: 480, Reward mean: 0.516, Reward std: 0.8400857099129827\n",
      "Step: 490, Reward mean: 0.622, Reward std: 0.7714376189945626\n",
      "Step: 500, Reward mean: 0.564, Reward std: 0.8061662359588128\n",
      "Step: 510, Reward mean: 0.564, Reward std: 0.8135748275358575\n",
      "Step: 520, Reward mean: 0.594, Reward std: 0.793198587996726\n",
      "Step: 530, Reward mean: 0.626, Reward std: 0.752412121114486\n",
      "Step: 540, Reward mean: 0.668, Reward std: 0.7251041304529992\n",
      "Step: 550, Reward mean: 0.664, Reward std: 0.7315080313981522\n",
      "Step: 560, Reward mean: 0.694, Reward std: 0.7101858911580826\n",
      "Step: 570, Reward mean: 0.714, Reward std: 0.6664863089366503\n",
      "Step: 580, Reward mean: 0.75, Reward std: 0.641482657598785\n",
      "Step: 590, Reward mean: 0.748, Reward std: 0.636\n",
      "Step: 600, Reward mean: 0.742, Reward std: 0.6507196016718721\n",
      "Step: 610, Reward mean: 0.768, Reward std: 0.6214306075500304\n",
      "Step: 620, Reward mean: 0.746, Reward std: 0.6399093685827704\n",
      "Step: 630, Reward mean: 0.722, Reward std: 0.660844913727873\n",
      "Step: 640, Reward mean: 0.736, Reward std: 0.6529195968877025\n",
      "Step: 650, Reward mean: 0.696, Reward std: 0.6925200358112392\n",
      "Step: 660, Reward mean: 0.658, Reward std: 0.7328274012344244\n",
      "Step: 670, Reward mean: 0.622, Reward std: 0.7662349509125774\n",
      "Step: 680, Reward mean: 0.712, Reward std: 0.6701164077979288\n",
      "Step: 690, Reward mean: 0.63, Reward std: 0.762299153875957\n",
      "Step: 700, Reward mean: 0.72, Reward std: 0.6881860213634101\n",
      "Step: 710, Reward mean: 0.71, Reward std: 0.6884039511798287\n",
      "Step: 720, Reward mean: 0.654, Reward std: 0.7336784036619859\n",
      "Step: 730, Reward mean: 0.7, Reward std: 0.7028513356322231\n",
      "Step: 740, Reward mean: 0.752, Reward std: 0.6438136376312635\n",
      "Step: 750, Reward mean: 0.702, Reward std: 0.6965601194441151\n",
      "Step: 760, Reward mean: 0.79, Reward std: 0.5948949487094339\n",
      "Step: 770, Reward mean: 0.808, Reward std: 0.5684505255516965\n",
      "Step: 780, Reward mean: 0.696, Reward std: 0.6925200358112392\n",
      "Step: 790, Reward mean: 0.712, Reward std: 0.66412047099905\n",
      "Step: 800, Reward mean: 0.69, Reward std: 0.7056202944927249\n",
      "Step: 810, Reward mean: 0.746, Reward std: 0.6430272155982202\n",
      "Step: 820, Reward mean: 0.712, Reward std: 0.6819501448053222\n",
      "Step: 830, Reward mean: 0.694, Reward std: 0.6959626426755965\n",
      "Step: 840, Reward mean: 0.684, Reward std: 0.7100309852393768\n",
      "Step: 850, Reward mean: 0.696, Reward std: 0.703977272360408\n",
      "Step: 860, Reward mean: 0.68, Reward std: 0.705407683541936\n",
      "Step: 870, Reward mean: 0.618, Reward std: 0.7720595831929036\n",
      "Step: 880, Reward mean: 0.612, Reward std: 0.7677603792850997\n",
      "Step: 890, Reward mean: 0.652, Reward std: 0.7475934724166604\n",
      "Step: 900, Reward mean: 0.666, Reward std: 0.7337874351608917\n",
      "Step: 910, Reward mean: 0.74, Reward std: 0.6514598989960932\n",
      "Step: 920, Reward mean: 0.67, Reward std: 0.7246378405796925\n",
      "Step: 930, Reward mean: 0.744, Reward std: 0.6437887852393827\n",
      "Step: 940, Reward mean: 0.702, Reward std: 0.661207985432723\n",
      "Step: 950, Reward mean: 0.614, Reward std: 0.7328055676644385\n",
      "Step: 960, Reward mean: 0.662, Reward std: 0.6983953035351826\n",
      "Step: 970, Reward mean: 0.66, Reward std: 0.6902173570694959\n",
      "Step: 980, Reward mean: 0.658, Reward std: 0.7328274012344242\n",
      "Step: 990, Reward mean: 0.676, Reward std: 0.7035794198240879\n",
      "Step: 1000, Reward mean: 0.75, Reward std: 0.641482657598785\n"
     ]
    }
   ],
   "source": [
    "GAMMA = 0.98\n",
    "INITIAL_STEPS = 1024\n",
    "TRANSITIONS = 1000\n",
    "STEPS_PER_UPDATE = 4\n",
    "STEPS_PER_TARGET_UPDATE = STEPS_PER_UPDATE * 1000\n",
    "BATCH_SIZE = 128\n",
    "LEARNING_RATE = 1e-3\n",
    "REPLAY_BUFFER_MAX_LEN = 1000\n",
    "\n",
    "SEED = 13\n",
    "np.random.seed(SEED)\n",
    "random.seed(SEED)\n",
    "torch.manual_seed(SEED)\n",
    "torch.cuda.manual_seed(SEED)\n",
    "os.environ[\"PYTHONHASHSEED\"] = str(SEED)\n",
    "\n",
    "env = TicTacToe()\n",
    "dqn = DQN(state_dim=(N_ROWS, N_COLS), action_dim=N_ROWS * N_COLS)\n",
    "eps = 0.1\n",
    "state, _ = env.reset()\n",
    "\n",
    "for _ in range(INITIAL_STEPS):\n",
    "    action = env.sample_action()\n",
    "    \n",
    "    next_state, reward, done, _, _ = env.step(action)\n",
    "    # ход ноликов\n",
    "    if not done:\n",
    "        next_state, reward, done, _, _ = env.step(env.sample_action())\n",
    "    \n",
    "    dqn.consume_transition((state, action, next_state, reward, done))\n",
    "\n",
    "    state = next_state if not done else env.reset()[0]\n",
    "    \n",
    "for i in range(TRANSITIONS):\n",
    "    if random.random() < eps:\n",
    "        action = env.sample_action()\n",
    "    else:\n",
    "        possible_actions = list(map(env.int_from_action, env.getEmptySpaces()))\n",
    "        action = dqn.act(state, possible_actions)\n",
    "\n",
    "    next_state, reward, done, _, _ = env.step(action)\n",
    "    # ход ноликов\n",
    "    if not done:\n",
    "        next_state, reward, done, _, _ = env.step(env.sample_action())\n",
    "    \n",
    "    dqn.update((state, action, next_state, reward, done))\n",
    "\n",
    "    state = next_state if not done else env.reset()[0]\n",
    "\n",
    "    if (i + 1) % (TRANSITIONS//100) == 0:\n",
    "        rewards = evaluate_policy(dqn, 500)\n",
    "        print(f\"Step: {i+1}, Reward mean: {np.mean(rewards)}, Reward std: {np.std(rewards)}\")\n",
    "        dqn.save()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "977da2f4",
   "metadata": {},
   "source": [
    "# Учим нолики"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "4e211eda",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step: 20, Reward mean: 0.344, Reward std: 0.7678958262681208\n",
      "Step: 40, Reward mean: 0.324, Reward std: 0.9268354762308141\n",
      "Step: 60, Reward mean: 0.166, Reward std: 0.9687331934026003\n",
      "Step: 80, Reward mean: 0.238, Reward std: 0.9577870326956821\n",
      "Step: 100, Reward mean: 0.412, Reward std: 0.8162450612408017\n",
      "Step: 120, Reward mean: 0.236, Reward std: 0.862730548896931\n",
      "Step: 140, Reward mean: 0.242, Reward std: 0.8691582134456305\n",
      "Step: 160, Reward mean: 0.272, Reward std: 0.8209847745238641\n",
      "Step: 180, Reward mean: 0.18, Reward std: 0.8964373932405988\n",
      "Step: 200, Reward mean: 0.102, Reward std: 0.91847482273604\n",
      "Step: 220, Reward mean: 0.26, Reward std: 0.8440379138403676\n",
      "Step: 240, Reward mean: 0.2, Reward std: 0.8579044235810886\n",
      "Step: 260, Reward mean: 0.084, Reward std: 0.7516275673496816\n",
      "Step: 280, Reward mean: 0.044, Reward std: 0.7733459769081364\n",
      "Step: 300, Reward mean: -0.094, Reward std: 0.8106565240593577\n",
      "Step: 320, Reward mean: -0.078, Reward std: 0.7693607736296413\n",
      "Step: 340, Reward mean: -0.288, Reward std: 0.7880710627855841\n",
      "Step: 360, Reward mean: -0.168, Reward std: 0.7508501847905479\n",
      "Step: 380, Reward mean: -0.188, Reward std: 0.7801640853051363\n",
      "Step: 400, Reward mean: -0.198, Reward std: 0.7840892806307199\n",
      "Step: 420, Reward mean: -0.252, Reward std: 0.7902505931664968\n",
      "Step: 440, Reward mean: -0.256, Reward std: 0.7658093757587459\n",
      "Step: 460, Reward mean: -0.172, Reward std: 0.8452313292821084\n",
      "Step: 480, Reward mean: -0.194, Reward std: 0.824841803014372\n",
      "Step: 500, Reward mean: -0.208, Reward std: 0.856\n",
      "Step: 520, Reward mean: -0.176, Reward std: 0.9061037468193144\n",
      "Step: 540, Reward mean: -0.18, Reward std: 0.86\n",
      "Step: 560, Reward mean: -0.17, Reward std: 0.8950418984606252\n",
      "Step: 580, Reward mean: -0.076, Reward std: 0.9023436152597302\n",
      "Step: 600, Reward mean: -0.04, Reward std: 0.9350935782048767\n",
      "Step: 620, Reward mean: -0.128, Reward std: 0.9141203421869575\n",
      "Step: 640, Reward mean: -0.052, Reward std: 0.9193998042201229\n",
      "Step: 660, Reward mean: 0.086, Reward std: 0.9244479433694469\n",
      "Step: 680, Reward mean: 0.198, Reward std: 0.8982182362878189\n",
      "Step: 700, Reward mean: -0.072, Reward std: 0.9004532192179668\n",
      "Step: 720, Reward mean: -0.116, Reward std: 0.9542242922919121\n",
      "Step: 740, Reward mean: -0.198, Reward std: 0.9288681284229748\n",
      "Step: 760, Reward mean: -0.132, Reward std: 0.9394551612503921\n",
      "Step: 780, Reward mean: -0.238, Reward std: 0.9128833441354924\n",
      "Step: 800, Reward mean: -0.242, Reward std: 0.9162074001010907\n",
      "Step: 820, Reward mean: -0.2, Reward std: 0.9423375191511798\n",
      "Step: 840, Reward mean: -0.386, Reward std: 0.8769287314257642\n",
      "Step: 860, Reward mean: -0.38, Reward std: 0.871550342780037\n",
      "Step: 880, Reward mean: -0.324, Reward std: 0.9203390679526757\n",
      "Step: 900, Reward mean: -0.42, Reward std: 0.7972452571197901\n",
      "Step: 920, Reward mean: -0.482, Reward std: 0.8060248135138272\n",
      "Step: 940, Reward mean: -0.486, Reward std: 0.8159681366327978\n",
      "Step: 960, Reward mean: -0.382, Reward std: 0.8809517580435378\n",
      "Step: 980, Reward mean: -0.266, Reward std: 0.8895189711299023\n",
      "Step: 1000, Reward mean: -0.166, Reward std: 0.9436334033935\n",
      "Step: 1020, Reward mean: -0.422, Reward std: 0.8763081649739435\n",
      "Step: 1040, Reward mean: -0.098, Reward std: 0.9799979591815484\n",
      "Step: 1060, Reward mean: -0.134, Reward std: 0.9571018754552725\n",
      "Step: 1080, Reward mean: -0.27, Reward std: 0.9534673565466203\n",
      "Step: 1100, Reward mean: -0.368, Reward std: 0.9276723559533289\n",
      "Step: 1120, Reward mean: -0.574, Reward std: 0.8176331695815673\n",
      "Step: 1140, Reward mean: -0.596, Reward std: 0.8029844332239574\n",
      "Step: 1160, Reward mean: -0.636, Reward std: 0.771689056550629\n",
      "Step: 1180, Reward mean: -0.47, Reward std: 0.8252878285786117\n",
      "Step: 1200, Reward mean: -0.448, Reward std: 0.8338441101309044\n",
      "Step: 1220, Reward mean: -0.306, Reward std: 0.9101450433859429\n",
      "Step: 1240, Reward mean: -0.664, Reward std: 0.7477325725150671\n",
      "Step: 1260, Reward mean: -0.6, Reward std: 0.7949842765740717\n",
      "Step: 1280, Reward mean: -0.448, Reward std: 0.8940335564172073\n",
      "Step: 1300, Reward mean: -0.32, Reward std: 0.947417542586161\n",
      "Step: 1320, Reward mean: -0.476, Reward std: 0.8794452797076121\n",
      "Step: 1340, Reward mean: -0.59, Reward std: 0.768049477572897\n",
      "Step: 1360, Reward mean: -0.61, Reward std: 0.7306846104852626\n",
      "Step: 1380, Reward mean: -0.412, Reward std: 0.9111838453352868\n",
      "Step: 1400, Reward mean: -0.388, Reward std: 0.9216593730874765\n",
      "Step: 1420, Reward mean: -0.616, Reward std: 0.7645547724002513\n",
      "Step: 1440, Reward mean: -0.572, Reward std: 0.7980075187615716\n",
      "Step: 1460, Reward mean: -0.664, Reward std: 0.7369559009872979\n",
      "Step: 1480, Reward mean: -0.56, Reward std: 0.8284926070883193\n",
      "Step: 1500, Reward mean: -0.454, Reward std: 0.8317956479809209\n",
      "Step: 1520, Reward mean: -0.446, Reward std: 0.838501043529464\n",
      "Step: 1540, Reward mean: -0.41, Reward std: 0.8682741502543998\n",
      "Step: 1560, Reward mean: -0.456, Reward std: 0.8602697251443875\n",
      "Step: 1580, Reward mean: -0.522, Reward std: 0.842327727194113\n",
      "Step: 1600, Reward mean: -0.562, Reward std: 0.757730822918007\n",
      "Step: 1620, Reward mean: -0.502, Reward std: 0.8160857798050398\n",
      "Step: 1640, Reward mean: -0.51, Reward std: 0.8160269603389338\n",
      "Step: 1660, Reward mean: -0.488, Reward std: 0.8613106292157319\n",
      "Step: 1680, Reward mean: -0.606, Reward std: 0.789153977370703\n",
      "Step: 1700, Reward mean: -0.642, Reward std: 0.7495572026203203\n",
      "Step: 1720, Reward mean: -0.638, Reward std: 0.6862623405083511\n",
      "Step: 1740, Reward mean: -0.606, Reward std: 0.7090585307293045\n",
      "Step: 1760, Reward mean: -0.644, Reward std: 0.685028466561792\n",
      "Step: 1780, Reward mean: -0.676, Reward std: 0.736901621656514\n",
      "Step: 1800, Reward mean: -0.636, Reward std: 0.7716890565506291\n",
      "Step: 1820, Reward mean: -0.616, Reward std: 0.787746152005835\n",
      "Step: 1840, Reward mean: -0.62, Reward std: 0.7846018098373213\n",
      "Step: 1860, Reward mean: -0.052, Reward std: 0.9784150448557096\n",
      "Step: 1880, Reward mean: -0.086, Reward std: 0.9831602107489908\n",
      "Step: 1900, Reward mean: -0.094, Reward std: 0.9783475864946978\n",
      "Step: 1920, Reward mean: -0.468, Reward std: 0.810540560366969\n",
      "Step: 1940, Reward mean: -0.124, Reward std: 0.9677933663752817\n",
      "Step: 1960, Reward mean: -0.7, Reward std: 0.714142842854285\n",
      "Step: 1980, Reward mean: -0.72, Reward std: 0.6939740629158988\n",
      "Step: 2000, Reward mean: -0.608, Reward std: 0.7939370252104382\n"
     ]
    }
   ],
   "source": [
    "GAMMA = 0.98\n",
    "INITIAL_STEPS = 1024\n",
    "TRANSITIONS = 2000\n",
    "STEPS_PER_UPDATE = 4\n",
    "STEPS_PER_TARGET_UPDATE = STEPS_PER_UPDATE * 1000\n",
    "BATCH_SIZE = 128\n",
    "LEARNING_RATE = 1e-3\n",
    "REPLAY_BUFFER_MAX_LEN = 1000\n",
    "\n",
    "SEED = 13\n",
    "np.random.seed(SEED)\n",
    "random.seed(SEED)\n",
    "torch.manual_seed(SEED)\n",
    "torch.cuda.manual_seed(SEED)\n",
    "os.environ[\"PYTHONHASHSEED\"] = str(SEED)\n",
    "\n",
    "env = TicTacToe()\n",
    "dqn = DQN(state_dim=(N_ROWS, N_COLS), action_dim=N_ROWS * N_COLS)\n",
    "eps = 0.1\n",
    "env.reset()\n",
    "\n",
    "state, reward, done, _, _ = env.step(env.sample_action())\n",
    "for _ in range(INITIAL_STEPS):\n",
    "    action = env.sample_action()\n",
    "    \n",
    "    next_state, reward, done, _, _ = env.step(action)\n",
    "    # ход крестиков\n",
    "    if not done:\n",
    "        next_state, reward, done, _, _ = env.step(env.sample_action())\n",
    "    \n",
    "    dqn.consume_transition((state, action, next_state, -reward, done))\n",
    "    \n",
    "    if done:\n",
    "        env.reset()\n",
    "        state, reward, done, _, _ = env.step(env.sample_action())\n",
    "    else:\n",
    "        state = next_state\n",
    "        \n",
    "for i in range(TRANSITIONS):\n",
    "    if random.random() < eps:\n",
    "        action = env.sample_action()\n",
    "    else:\n",
    "        possible_actions = list(map(env.int_from_action, env.getEmptySpaces()))\n",
    "        action = dqn.act(state, possible_actions)\n",
    "\n",
    "    next_state, reward, done, _, _ = env.step(action)\n",
    "    # ход крестиков\n",
    "    if not done:\n",
    "        next_state, reward, done, _, _ = env.step(env.sample_action())\n",
    "    \n",
    "    dqn.update((state, action, next_state, -reward, done))\n",
    "\n",
    "    if done:\n",
    "        env.reset()\n",
    "        state, reward, done, _, _ = env.step(env.sample_action())\n",
    "    else:\n",
    "        state = next_state\n",
    "\n",
    "    if (i + 1) % (TRANSITIONS//100) == 0:\n",
    "        rewards = evaluate_policy(dqn, 500, crosses=False)\n",
    "        print(f\"Step: {i+1}, Reward mean: {np.mean(rewards)}, Reward std: {np.std(rewards)}\")\n",
    "        dqn.save()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "087ecec0",
   "metadata": {},
   "source": [
    "# Учим крестики, 4x4x4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "5295dfa1",
   "metadata": {},
   "outputs": [],
   "source": [
    "class NeuralNetwork(nn.Module):\n",
    "    def __init__(self, state_dim, action_dim, hidden_dim=64):\n",
    "        super(self.__class__, self).__init__()\n",
    "        self.model = nn.Sequential(\n",
    "            nn.Conv2d(in_channels=1, out_channels=hidden_dim, kernel_size=(3, 3)),\n",
    "            nn.Flatten(),\n",
    "            nn.Linear(4 * hidden_dim, hidden_dim),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(hidden_dim, hidden_dim),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(hidden_dim, action_dim),\n",
    "            nn.Tanh()\n",
    "        )\n",
    "        \n",
    "    def forward(self, inp):       \n",
    "        out = self.model(inp)\n",
    "        return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "d3c7f2bc",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step: 20, Reward mean: -0.196, Reward std: 0.6399874998779274\n",
      "Step: 40, Reward mean: -0.244, Reward std: 0.7017577929741856\n",
      "Step: 60, Reward mean: -0.134, Reward std: 0.7563359042118786\n",
      "Step: 80, Reward mean: -0.214, Reward std: 0.7824346618088951\n",
      "Step: 100, Reward mean: -0.008, Reward std: 0.67818581524535\n",
      "Step: 120, Reward mean: 0.036, Reward std: 0.7005026766544151\n",
      "Step: 140, Reward mean: 0.086, Reward std: 0.6860058308790093\n",
      "Step: 160, Reward mean: 0.072, Reward std: 0.7367604766815331\n",
      "Step: 180, Reward mean: 0.312, Reward std: 0.8041492398802599\n",
      "Step: 200, Reward mean: 0.28, Reward std: 0.8009993757800314\n",
      "Step: 220, Reward mean: 0.218, Reward std: 0.7761932749000084\n",
      "Step: 240, Reward mean: 0.228, Reward std: 0.7874109473457935\n",
      "Step: 260, Reward mean: 0.3, Reward std: 0.7549834435270749\n",
      "Step: 280, Reward mean: 0.36, Reward std: 0.7552483035399683\n",
      "Step: 300, Reward mean: 0.404, Reward std: 0.7699246716400249\n",
      "Step: 320, Reward mean: 0.422, Reward std: 0.7509434066559211\n",
      "Step: 340, Reward mean: 0.3, Reward std: 0.7987490219086343\n",
      "Step: 360, Reward mean: 0.186, Reward std: 0.7793612769441397\n",
      "Step: 380, Reward mean: 0.23, Reward std: 0.8277076778670113\n",
      "Step: 400, Reward mean: 0.164, Reward std: 0.8006897026938712\n",
      "Step: 420, Reward mean: 0.244, Reward std: 0.8052726246433564\n",
      "Step: 440, Reward mean: 0.224, Reward std: 0.7653914031395964\n",
      "Step: 460, Reward mean: 0.296, Reward std: 0.805222950492595\n",
      "Step: 480, Reward mean: 0.326, Reward std: 0.7948106692791687\n",
      "Step: 500, Reward mean: 0.298, Reward std: 0.7517951848741783\n",
      "Step: 520, Reward mean: 0.178, Reward std: 0.7786629566121661\n",
      "Step: 540, Reward mean: 0.212, Reward std: 0.784255060551094\n",
      "Step: 560, Reward mean: 0.308, Reward std: 0.7649418278535957\n",
      "Step: 580, Reward mean: 0.27, Reward std: 0.7463913182774836\n",
      "Step: 600, Reward mean: 0.17, Reward std: 0.7675285010994706\n",
      "Step: 620, Reward mean: 0.16, Reward std: 0.7709734106958552\n",
      "Step: 640, Reward mean: 0.22, Reward std: 0.7613146524269713\n",
      "Step: 660, Reward mean: 0.178, Reward std: 0.7939244296531\n",
      "Step: 680, Reward mean: 0.274, Reward std: 0.7739018025563709\n",
      "Step: 700, Reward mean: 0.24, Reward std: 0.7939773296511684\n",
      "Step: 720, Reward mean: 0.24, Reward std: 0.8089499366462675\n",
      "Step: 740, Reward mean: 0.344, Reward std: 0.7600421040968717\n",
      "Step: 760, Reward mean: 0.356, Reward std: 0.7571419946086732\n",
      "Step: 780, Reward mean: 0.374, Reward std: 0.7811043464224227\n",
      "Step: 800, Reward mean: 0.31, Reward std: 0.788606365685695\n",
      "Step: 820, Reward mean: 0.344, Reward std: 0.744086016533035\n",
      "Step: 840, Reward mean: 0.33, Reward std: 0.7410128204019146\n",
      "Step: 860, Reward mean: 0.368, Reward std: 0.737953928101206\n",
      "Step: 880, Reward mean: 0.22, Reward std: 0.79724525711979\n",
      "Step: 900, Reward mean: 0.234, Reward std: 0.8168500474383288\n",
      "Step: 920, Reward mean: 0.33, Reward std: 0.7490660852021002\n",
      "Step: 940, Reward mean: 0.236, Reward std: 0.7431715818032872\n",
      "Step: 960, Reward mean: 0.316, Reward std: 0.7322185466102317\n",
      "Step: 980, Reward mean: 0.336, Reward std: 0.7423637922204989\n",
      "Step: 1000, Reward mean: 0.3, Reward std: 0.7733045971672482\n",
      "Step: 1020, Reward mean: 0.318, Reward std: 0.7462412478548743\n",
      "Step: 1040, Reward mean: 0.244, Reward std: 0.7697168310489254\n",
      "Step: 1060, Reward mean: 0.27, Reward std: 0.7543871685016919\n",
      "Step: 1080, Reward mean: 0.35, Reward std: 0.7426304599193331\n",
      "Step: 1100, Reward mean: 0.322, Reward std: 0.7630963241950521\n",
      "Step: 1120, Reward mean: 0.318, Reward std: 0.7777377450014884\n",
      "Step: 1140, Reward mean: 0.288, Reward std: 0.7931305062850628\n",
      "Step: 1160, Reward mean: 0.356, Reward std: 0.7676353300884478\n",
      "Step: 1180, Reward mean: 0.392, Reward std: 0.7837958917983686\n",
      "Step: 1200, Reward mean: 0.524, Reward std: 0.7248613660556066\n",
      "Step: 1220, Reward mean: 0.45, Reward std: 0.7453187237685633\n",
      "Step: 1240, Reward mean: 0.432, Reward std: 0.7545700762685994\n",
      "Step: 1260, Reward mean: 0.598, Reward std: 0.6755708697094628\n",
      "Step: 1280, Reward mean: 0.47, Reward std: 0.7218725649309579\n",
      "Step: 1300, Reward mean: 0.448, Reward std: 0.7397945660789892\n",
      "Step: 1320, Reward mean: 0.42, Reward std: 0.74\n",
      "Step: 1340, Reward mean: 0.348, Reward std: 0.7712950148937824\n",
      "Step: 1360, Reward mean: 0.408, Reward std: 0.7573215961531798\n",
      "Step: 1380, Reward mean: 0.374, Reward std: 0.7417034447809987\n",
      "Step: 1400, Reward mean: 0.36, Reward std: 0.7525955088890711\n",
      "Step: 1420, Reward mean: 0.308, Reward std: 0.7596946755111555\n",
      "Step: 1440, Reward mean: 0.334, Reward std: 0.7526247404915679\n",
      "Step: 1460, Reward mean: 0.31, Reward std: 0.705620294492725\n",
      "Step: 1480, Reward mean: 0.33, Reward std: 0.7649182962905254\n",
      "Step: 1500, Reward mean: 0.374, Reward std: 0.7443950564048636\n",
      "Step: 1520, Reward mean: 0.292, Reward std: 0.7090387859630811\n",
      "Step: 1540, Reward mean: 0.282, Reward std: 0.6829904830962142\n",
      "Step: 1560, Reward mean: 0.386, Reward std: 0.7300712294016249\n",
      "Step: 1580, Reward mean: 0.392, Reward std: 0.7552059321800909\n",
      "Step: 1600, Reward mean: 0.416, Reward std: 0.7556083641675759\n",
      "Step: 1620, Reward mean: 0.352, Reward std: 0.7694777449673251\n",
      "Step: 1640, Reward mean: 0.292, Reward std: 0.771191286257826\n",
      "Step: 1660, Reward mean: 0.248, Reward std: 0.7787785307775246\n",
      "Step: 1680, Reward mean: 0.33, Reward std: 0.7570336848516056\n",
      "Step: 1700, Reward mean: 0.39, Reward std: 0.7279423054061358\n",
      "Step: 1720, Reward mean: 0.392, Reward std: 0.7337138406763225\n",
      "Step: 1740, Reward mean: 0.384, Reward std: 0.746018766519985\n",
      "Step: 1760, Reward mean: 0.396, Reward std: 0.7531161928945626\n",
      "Step: 1780, Reward mean: 0.22, Reward std: 0.7846018098373212\n",
      "Step: 1800, Reward mean: 0.206, Reward std: 0.7820255750293593\n",
      "Step: 1820, Reward mean: 0.298, Reward std: 0.8106762609081383\n",
      "Step: 1840, Reward mean: 0.328, Reward std: 0.7774419592484059\n",
      "Step: 1860, Reward mean: 0.374, Reward std: 0.7862086237125614\n",
      "Step: 1880, Reward mean: 0.302, Reward std: 0.7634107675426121\n",
      "Step: 1900, Reward mean: 0.254, Reward std: 0.7755539955412519\n",
      "Step: 1920, Reward mean: 0.236, Reward std: 0.795175452337407\n",
      "Step: 1940, Reward mean: 0.168, Reward std: 0.79484338079901\n",
      "Step: 1960, Reward mean: 0.142, Reward std: 0.7627817512237691\n",
      "Step: 1980, Reward mean: 0.27, Reward std: 0.7543871685016919\n",
      "Step: 2000, Reward mean: 0.372, Reward std: 0.7082485439448499\n"
     ]
    }
   ],
   "source": [
    "N_ROWS, N_COLS, N_WIN = 4, 4, 4\n",
    "\n",
    "GAMMA = 0.98\n",
    "INITIAL_STEPS = 1024\n",
    "TRANSITIONS = 2000\n",
    "STEPS_PER_UPDATE = 4\n",
    "STEPS_PER_TARGET_UPDATE = STEPS_PER_UPDATE * 1000\n",
    "BATCH_SIZE = 128\n",
    "LEARNING_RATE = 1e-3\n",
    "REPLAY_BUFFER_MAX_LEN = 1000\n",
    "\n",
    "SEED = 13\n",
    "np.random.seed(SEED)\n",
    "random.seed(SEED)\n",
    "torch.manual_seed(SEED)\n",
    "torch.cuda.manual_seed(SEED)\n",
    "os.environ[\"PYTHONHASHSEED\"] = str(SEED)\n",
    "\n",
    "env = TicTacToe(n_rows=N_ROWS, n_cols=N_COLS, n_win=N_WIN)\n",
    "dqn = DQN(state_dim=(N_ROWS, N_COLS), action_dim=N_ROWS * N_COLS)\n",
    "eps = 0.1\n",
    "state, _ = env.reset()\n",
    "\n",
    "for _ in range(INITIAL_STEPS):\n",
    "    action = env.sample_action()\n",
    "    \n",
    "    next_state, reward, done, _, _ = env.step(action)\n",
    "    # ход ноликов\n",
    "    if not done:\n",
    "        next_state, reward, done, _, _ = env.step(env.sample_action())\n",
    "    \n",
    "    dqn.consume_transition((state, action, next_state, reward, done))\n",
    "\n",
    "    state = next_state if not done else env.reset()[0]\n",
    "    \n",
    "for i in range(TRANSITIONS):\n",
    "    if random.random() < eps:\n",
    "        action = env.sample_action()\n",
    "    else:\n",
    "        possible_actions = list(map(env.int_from_action, env.getEmptySpaces()))\n",
    "        action = dqn.act(state, possible_actions)\n",
    "\n",
    "    next_state, reward, done, _, _ = env.step(action)\n",
    "    # ход ноликов\n",
    "    if not done:\n",
    "        next_state, reward, done, _, _ = env.step(env.sample_action())\n",
    "    \n",
    "    dqn.update((state, action, next_state, reward, done))\n",
    "\n",
    "    state = next_state if not done else env.reset()[0]\n",
    "\n",
    "    if (i + 1) % (TRANSITIONS//100) == 0:\n",
    "        rewards = evaluate_policy(dqn, 500)\n",
    "        print(f\"Step: {i+1}, Reward mean: {np.mean(rewards)}, Reward std: {np.std(rewards)}\")\n",
    "        dqn.save()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "683b50ef",
   "metadata": {},
   "source": [
    "# Учим нолики 4x4x4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "fa29473e",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step: 20, Reward mean: 0.052, Reward std: 0.759799973677283\n",
      "Step: 40, Reward mean: 0.044, Reward std: 0.768156234108661\n",
      "Step: 60, Reward mean: 0.166, Reward std: 0.7144536374041355\n",
      "Step: 80, Reward mean: 0.058, Reward std: 0.6592692924746306\n",
      "Step: 100, Reward mean: -0.074, Reward std: 0.698944919145994\n",
      "Step: 120, Reward mean: 0.01, Reward std: 0.7758221445666527\n",
      "Step: 140, Reward mean: -0.006, Reward std: 0.7937027151270178\n",
      "Step: 160, Reward mean: -0.064, Reward std: 0.7238121303211213\n",
      "Step: 180, Reward mean: -0.242, Reward std: 0.7179387160475467\n",
      "Step: 200, Reward mean: -0.07, Reward std: 0.7190966555338718\n",
      "Step: 220, Reward mean: -0.042, Reward std: 0.764353321442381\n",
      "Step: 240, Reward mean: -0.134, Reward std: 0.8050118011557346\n",
      "Step: 260, Reward mean: -0.246, Reward std: 0.8059057016797933\n",
      "Step: 280, Reward mean: -0.204, Reward std: 0.8236406983630665\n",
      "Step: 300, Reward mean: -0.18, Reward std: 0.7896834808959854\n",
      "Step: 320, Reward mean: -0.218, Reward std: 0.7684243619251019\n",
      "Step: 340, Reward mean: -0.132, Reward std: 0.7788298915681139\n",
      "Step: 360, Reward mean: -0.15, Reward std: 0.7971825386948713\n",
      "Step: 380, Reward mean: -0.142, Reward std: 0.8257336132191786\n",
      "Step: 400, Reward mean: 0.068, Reward std: 0.8193753718534625\n",
      "Step: 420, Reward mean: -0.022, Reward std: 0.8541170879920387\n",
      "Step: 440, Reward mean: -0.124, Reward std: 0.8103233922330022\n",
      "Step: 460, Reward mean: -0.23, Reward std: 0.7906326580656784\n",
      "Step: 480, Reward mean: -0.056, Reward std: 0.8104714677272727\n",
      "Step: 500, Reward mean: -0.132, Reward std: 0.7684894273833572\n",
      "Step: 520, Reward mean: -0.092, Reward std: 0.7691137757185215\n",
      "Step: 540, Reward mean: -0.148, Reward std: 0.8474054519531957\n",
      "Step: 560, Reward mean: -0.086, Reward std: 0.8334290611683757\n",
      "Step: 580, Reward mean: -0.136, Reward std: 0.8182322408705246\n",
      "Step: 600, Reward mean: -0.172, Reward std: 0.8333162664919005\n",
      "Step: 620, Reward mean: -0.004, Reward std: 0.8625450712861329\n",
      "Step: 640, Reward mean: -0.11, Reward std: 0.8865100112237876\n",
      "Step: 660, Reward mean: -0.064, Reward std: 0.8740160181598504\n",
      "Step: 680, Reward mean: -0.354, Reward std: 0.8629507517813516\n",
      "Step: 700, Reward mean: -0.33, Reward std: 0.8562125904236635\n",
      "Step: 720, Reward mean: -0.326, Reward std: 0.8507196953168535\n",
      "Step: 740, Reward mean: -0.294, Reward std: 0.8411682352538046\n",
      "Step: 760, Reward mean: -0.16, Reward std: 0.8799999999999999\n",
      "Step: 780, Reward mean: -0.122, Reward std: 0.902837748435454\n",
      "Step: 800, Reward mean: -0.49, Reward std: 0.7860661549767932\n",
      "Step: 820, Reward mean: 0.06, Reward std: 0.782559901860554\n",
      "Step: 840, Reward mean: 0.096, Reward std: 0.7789634137749989\n",
      "Step: 860, Reward mean: -0.008, Reward std: 0.7822633827554503\n",
      "Step: 880, Reward mean: 0.04, Reward std: 0.7863841300534999\n",
      "Step: 900, Reward mean: -0.04, Reward std: 0.816333265278342\n",
      "Step: 920, Reward mean: -0.08, Reward std: 0.795989949685296\n",
      "Step: 940, Reward mean: -0.032, Reward std: 0.8312496616540664\n",
      "Step: 960, Reward mean: -0.434, Reward std: 0.7884440373292196\n",
      "Step: 980, Reward mean: -0.378, Reward std: 0.7609967148417923\n",
      "Step: 1000, Reward mean: -0.424, Reward std: 0.7538063411778916\n",
      "Step: 1020, Reward mean: 0.056, Reward std: 0.7828563086544043\n",
      "Step: 1040, Reward mean: 0.05, Reward std: 0.7690903718029501\n",
      "Step: 1060, Reward mean: -0.004, Reward std: 0.8123939930846362\n",
      "Step: 1080, Reward mean: -0.428, Reward std: 0.767343469379912\n",
      "Step: 1100, Reward mean: -0.136, Reward std: 0.7573004687704875\n",
      "Step: 1120, Reward mean: -0.13, Reward std: 0.7855571271397134\n",
      "Step: 1140, Reward mean: 0.0, Reward std: 0.7429670248402684\n",
      "Step: 1160, Reward mean: -0.046, Reward std: 0.7320409824593156\n",
      "Step: 1180, Reward mean: -0.054, Reward std: 0.7287551028980861\n",
      "Step: 1200, Reward mean: -0.064, Reward std: 0.7210436879967815\n",
      "Step: 1220, Reward mean: 0.068, Reward std: 0.7664045928881168\n",
      "Step: 1240, Reward mean: 0.02, Reward std: 0.7613146524269712\n",
      "Step: 1260, Reward mean: -0.112, Reward std: 0.7262616608358176\n",
      "Step: 1280, Reward mean: -0.076, Reward std: 0.7551317765794259\n",
      "Step: 1300, Reward mean: -0.366, Reward std: 0.8000274995273601\n",
      "Step: 1320, Reward mean: -0.32, Reward std: 0.7934733769950949\n",
      "Step: 1340, Reward mean: -0.118, Reward std: 0.7823528615656749\n",
      "Step: 1360, Reward mean: -0.068, Reward std: 0.8290814194034263\n",
      "Step: 1380, Reward mean: -0.6, Reward std: 0.6782329983125269\n",
      "Step: 1400, Reward mean: -0.158, Reward std: 0.7190521538803705\n",
      "Step: 1420, Reward mean: -0.036, Reward std: 0.8937024113204574\n",
      "Step: 1440, Reward mean: -0.432, Reward std: 0.7677082779285372\n",
      "Step: 1460, Reward mean: -0.322, Reward std: 0.8039378085399392\n",
      "Step: 1480, Reward mean: -0.358, Reward std: 0.7936220763058448\n",
      "Step: 1500, Reward mean: -0.23, Reward std: 0.754387168501692\n",
      "Step: 1520, Reward mean: -0.4, Reward std: 0.7848566748139434\n",
      "Step: 1540, Reward mean: -0.308, Reward std: 0.8325478965200741\n",
      "Step: 1560, Reward mean: -0.264, Reward std: 0.8235921320653811\n",
      "Step: 1580, Reward mean: -0.264, Reward std: 0.803930345241427\n",
      "Step: 1600, Reward mean: -0.262, Reward std: 0.8302746533527324\n",
      "Step: 1620, Reward mean: 0.162, Reward std: 0.8147122191301662\n",
      "Step: 1640, Reward mean: 0.016, Reward std: 0.850731449988773\n",
      "Step: 1660, Reward mean: 0.03, Reward std: 0.8130805618141415\n",
      "Step: 1680, Reward mean: 0.05, Reward std: 0.8218880702382776\n",
      "Step: 1700, Reward mean: 0.074, Reward std: 0.7877334574588031\n",
      "Step: 1720, Reward mean: 0.052, Reward std: 0.8302385199447204\n",
      "Step: 1740, Reward mean: -0.494, Reward std: 0.714117637367962\n",
      "Step: 1760, Reward mean: 0.082, Reward std: 0.819314347488191\n",
      "Step: 1780, Reward mean: 0.022, Reward std: 0.8034401035547081\n",
      "Step: 1800, Reward mean: -0.072, Reward std: 0.8092070192478561\n",
      "Step: 1820, Reward mean: -0.072, Reward std: 0.8311534154414575\n",
      "Step: 1840, Reward mean: -0.08, Reward std: 0.8280096617793781\n",
      "Step: 1860, Reward mean: -0.494, Reward std: 0.7361820427041126\n",
      "Step: 1880, Reward mean: -0.516, Reward std: 0.7250820643209981\n",
      "Step: 1900, Reward mean: 0.006, Reward std: 0.8258111164182763\n",
      "Step: 1920, Reward mean: -0.258, Reward std: 0.7793818063054846\n",
      "Step: 1940, Reward mean: -0.542, Reward std: 0.712906726016805\n",
      "Step: 1960, Reward mean: -0.43, Reward std: 0.7931582439841371\n",
      "Step: 1980, Reward mean: -0.392, Reward std: 0.8014586701758238\n",
      "Step: 2000, Reward mean: -0.398, Reward std: 0.789680948231626\n"
     ]
    }
   ],
   "source": [
    "GAMMA = 0.98\n",
    "INITIAL_STEPS = 1024\n",
    "TRANSITIONS = 2000\n",
    "STEPS_PER_UPDATE = 4\n",
    "STEPS_PER_TARGET_UPDATE = STEPS_PER_UPDATE * 1000\n",
    "BATCH_SIZE = 128\n",
    "LEARNING_RATE = 1e-3\n",
    "REPLAY_BUFFER_MAX_LEN = 1000\n",
    "\n",
    "SEED = 13\n",
    "np.random.seed(SEED)\n",
    "random.seed(SEED)\n",
    "torch.manual_seed(SEED)\n",
    "torch.cuda.manual_seed(SEED)\n",
    "os.environ[\"PYTHONHASHSEED\"] = str(SEED)\n",
    "\n",
    "env = TicTacToe(n_rows=N_ROWS, n_cols=N_COLS, n_win=N_WIN)\n",
    "dqn = DQN(state_dim=(N_ROWS, N_COLS), action_dim=N_ROWS * N_COLS)\n",
    "eps = 0.1\n",
    "env.reset()\n",
    "\n",
    "state, reward, done, _, _ = env.step(env.sample_action())\n",
    "for _ in range(INITIAL_STEPS):\n",
    "    action = env.sample_action()\n",
    "    \n",
    "    next_state, reward, done, _, _ = env.step(action)\n",
    "    # ход крестиков\n",
    "    if not done:\n",
    "        next_state, reward, done, _, _ = env.step(env.sample_action())\n",
    "    \n",
    "    dqn.consume_transition((state, action, next_state, -reward, done))\n",
    "    \n",
    "    if done:\n",
    "        env.reset()\n",
    "        state, reward, done, _, _ = env.step(env.sample_action())\n",
    "    else:\n",
    "        state = next_state\n",
    "        \n",
    "for i in range(TRANSITIONS):\n",
    "    if random.random() < eps:\n",
    "        action = env.sample_action()\n",
    "    else:\n",
    "        possible_actions = list(map(env.int_from_action, env.getEmptySpaces()))\n",
    "        action = dqn.act(state, possible_actions)\n",
    "\n",
    "    next_state, reward, done, _, _ = env.step(action)\n",
    "    # ход крестиков\n",
    "    if not done:\n",
    "        next_state, reward, done, _, _ = env.step(env.sample_action())\n",
    "    \n",
    "    dqn.update((state, action, next_state, -reward, done))\n",
    "\n",
    "    if done:\n",
    "        env.reset()\n",
    "        state, reward, done, _, _ = env.step(env.sample_action())\n",
    "    else:\n",
    "        state = next_state\n",
    "\n",
    "    if (i + 1) % (TRANSITIONS//100) == 0:\n",
    "        rewards = evaluate_policy(dqn, 500, crosses=False)\n",
    "        print(f\"Step: {i+1}, Reward mean: {np.mean(rewards)}, Reward std: {np.std(rewards)}\")\n",
    "        dqn.save()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
